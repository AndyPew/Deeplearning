import torch
from torch import nn
from torch.nn import functional as F
from torch import nn
from abc import abstractmethod
import os
import torch
from torch import Tensor
from pathlib import Path
from typing import List, Optional, Sequence, Union, Any, Callable
#from torchvision.datasets.folder import default_loader
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
import torch.optim as optim
import torchvision
import numpy as np
import imageio
import rawpy
import sys



class denoising_model(nn.Module):
  def __init__(self):
    super(denoising_model,self).__init__()
    self.encoder=nn.Sequential(
                  nn.Linear(28*28,256),
                  nn.ReLU(True),
                  nn.Linear(256,128),
                  nn.ReLU(True),
                  nn.Linear(128,64),
                  nn.ReLU(True)
        
                  )
    
    self.decoder=nn.Sequential(
                  nn.Linear(64,128),
                  nn.ReLU(True),
                  nn.Linear(128,256),
                  nn.ReLU(True),
                  nn.Linear(256,28*28),
                  nn.Sigmoid(),
                  )
    
 
  def forward(self,x):
    x=self.encoder(x)
    x=self.decoder(x)
    
    return x
    
    
def initialize_weights(model):
    for m in model.modules():  
        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):    
            nn.init.normal_(m.weight.data, 0.0, 0.02)   
    
model=denoising_model().to(device)
criterion=nn.MSELoss()
optimizer=optim.SGD(model.parameters(),lr=0.01,weight_decay=1e-5)


epochs=120
l=len(trainloader)
losslist=list()
epochloss=0
running_loss=0


#hyperparameter
device = "cuda" if torch.cuda.is_available() else "cpu"
IN_CHANNELS = 3
LATENT_DIM = 100
IMG_CHANNELS = 3
EPOCH = 10
LR = 1e-4
BATCH_SIZE = 64


#Image transformation
transforms = transforms.Compose([#transforms.RandomHorizontalFlip(),
                                    #transforms.CenterCrop(148),
                                    #transforms.Resize(),
                                    transforms.ToTensor(),
                                    transforms.Normalize
                                     ([0.5 for _ in range(IMG_CHANNELS)],[0.5 for _ in range(IMG_CHANNELS)])])


#dataloading
def extract_bayer_channels(raw):

    ch_B  = raw[1::2, 1::2]
    ch_Gb = raw[0::2, 1::2]
    ch_R  = raw[0::2, 0::2]
    ch_Gr = raw[1::2, 0::2]

    return ch_R, ch_Gr, ch_B, ch_Gb


#Raw data converted to operational data(jpeg)
file_path = './testset/Hearthstone Screenshot 02-12-21 18.12.55.png'
dir_path = './1'
file_list =  os.listdir(dir_path)

for file in file_list:    
    with rawpy.imread(f'./1/{file}') as raw_image:         
        rgb = raw_image.postprocess(use_camera_wb=True, no_auto_bright=True, output_bps=16)
        imageio.imsave(f'./testset/{file}.jpeg', rgb)

#img = ImageLoader(data_path="./testset", split="train", transform=transforms)


#model initializing
VAE = VanillaVAE(IN_CHANNELS,LATENT_DIM).to(device)
initialize_weights(VAE)
VAE.train()



#optimizer called in training is enough
optim_VAE = optim.Adam(VAE.parameters(), lr=LR, betas=(0.0, 0.9),weight_decay=0.99)

for epoch in range(EPOCH):
    for pic in enumerate(img):
        pic = pic.yo(device)
        reconstructed_img, mu, logvar = VAE(pic) 
        loss = loss_function(reconstructed_img, pic, mu, logvar)
        VAE.zero_grad()
        loss=backward()
        optim_VAE.step()
        print(f'loss:{loss}')

for epoch in range(epochs):
  
  print("Entering Epoch: ",epoch)
  for dirty,clean,label in tqdm((trainloader)):
    
    
    dirty=dirty.view(dirty.size(0),-1).type(torch.FloatTensor)
    clean=clean.view(clean.size(0),-1).type(torch.FloatTensor)
    dirty,clean=dirty.to(device),clean.to(device)
    
    
    
    #-----------------Forward Pass----------------------
    output=model(dirty)
    loss=criterion(output,clean)
    #-----------------Backward Pass---------------------
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    running_loss+=loss.item()
    epochloss+=loss.item()
  #-----------------Log-------------------------------
  losslist.append(running_loss/l)
  running_loss=0
  print("======> epoch: {}/{}, Loss:{}".format(epoch,epochs,loss.item()))
